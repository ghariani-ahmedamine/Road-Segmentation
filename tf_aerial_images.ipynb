{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Baseline for machine learning project on road segmentation.<br>\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss<br>\n",
    "Credits: Aurelien Lucchi, ETH ZÃ¼rich<br>\n",
    "This was last tested with TensorFlow 1.13.2, which is not completely up to date.<br>\n",
    "To 'downgrade': pip install --upgrade tensorflow==1.13.2<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-unet\n",
      "  Downloading keras_unet-0.1.2-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: keras-unet\n",
      "Successfully installed keras-unet-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.models import vanilla_unet\n",
    "\n",
    "model = vanilla_unet(input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet_collection import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.python.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3  # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 20\n",
    "VALIDATION_SIZE = 5  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16  # 64\n",
    "NUM_EPOCHS = 100\n",
    "RESTORE_MODEL = False  # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set image patch size in pixels<br>\n",
    "IMG_PATCH_SIZE should be a multiple of 4<br>\n",
    "image size should be an integer multiple of this number!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list(tf.flags.FLAGS):\n",
    "    delattr(tf.flags.FLAGS,name)\n",
    "tf.flags.DEFINE_string('train_dir', '/tmp/segment_aerial_images',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract patches from a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print('File ' + image_filename + ' does not exist')\n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "    return numpy.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/training/images/satImage_001.png\n",
      "Loading data/training/images/satImage_002.png\n",
      "Loading data/training/images/satImage_003.png\n",
      "Loading data/training/images/satImage_004.png\n",
      "Loading data/training/images/satImage_005.png\n",
      "Loading data/training/images/satImage_006.png\n",
      "Loading data/training/images/satImage_007.png\n",
      "Loading data/training/images/satImage_008.png\n",
      "Loading data/training/images/satImage_009.png\n",
      "Loading data/training/images/satImage_010.png\n",
      "Loading data/training/images/satImage_011.png\n",
      "Loading data/training/images/satImage_012.png\n",
      "Loading data/training/images/satImage_013.png\n",
      "Loading data/training/images/satImage_014.png\n",
      "Loading data/training/images/satImage_015.png\n",
      "Loading data/training/images/satImage_016.png\n",
      "Loading data/training/images/satImage_017.png\n",
      "Loading data/training/images/satImage_018.png\n",
      "Loading data/training/images/satImage_019.png\n",
      "Loading data/training/images/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "train_data = extract_data('data/training/images/', TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 16, 16, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a label to a patch v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25  # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:  # road\n",
    "        return [0, 1]\n",
    "    else:  # bgrd\n",
    "        return [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract label images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print('File ' + image_filename + ' does not exist')\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/training/groundtruth/satImage_001.png\n",
      "Loading data/training/groundtruth/satImage_002.png\n",
      "Loading data/training/groundtruth/satImage_003.png\n",
      "Loading data/training/groundtruth/satImage_004.png\n",
      "Loading data/training/groundtruth/satImage_005.png\n",
      "Loading data/training/groundtruth/satImage_006.png\n",
      "Loading data/training/groundtruth/satImage_007.png\n",
      "Loading data/training/groundtruth/satImage_008.png\n",
      "Loading data/training/groundtruth/satImage_009.png\n",
      "Loading data/training/groundtruth/satImage_010.png\n",
      "Loading data/training/groundtruth/satImage_011.png\n",
      "Loading data/training/groundtruth/satImage_012.png\n",
      "Loading data/training/groundtruth/satImage_013.png\n",
      "Loading data/training/groundtruth/satImage_014.png\n",
      "Loading data/training/groundtruth/satImage_015.png\n",
      "Loading data/training/groundtruth/satImage_016.png\n",
      "Loading data/training/groundtruth/satImage_017.png\n",
      "Loading data/training/groundtruth/satImage_018.png\n",
      "Loading data/training/groundtruth/satImage_019.png\n",
      "Loading data/training/groundtruth/satImage_020.png\n"
     ]
    }
   ],
   "source": [
    "train_labels = extract_data('data/training/groundtruth/', TRAINING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and 1-hot labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1)) /\n",
    "        predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write predictions from neural network to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_to_file(predictions, labels, filename):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    file = open(filename, \"w\")\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(0, n):\n",
    "        file.write(max_labels(i) + ' ' + max_predictions(i))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print predictions from neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(predictions, vbbels):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    print(str(max_labels) + ' ' + str(max_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert array of labels to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if labels[idx][0] > 0.5:  # bgrd\n",
    "                l = 0\n",
    "            else:\n",
    "                l = 1\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_images(img, gt_img):\n",
    "    n_channels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if n_channels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:, :, 0] = gt_img8\n",
    "        gt_img_3c[:, :, 1] = gt_img8\n",
    "        gt_img_3c[:, :, 2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "    color_mask[:, :, 0] = predicted_img*PIXEL_DEPTH\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "    data_dir = 'training/'\n",
    "    train_data_filename = data_dir + 'images/'\n",
    "    train_labels_filename = data_dir + 'groundtruth/' \n",
    "\n",
    "    # Extract it into numpy arrays.\n",
    "    train_data = extract_data(train_data_filename, TRAINING_SIZE)\n",
    "    train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    c0 = 0  # bgrd\n",
    "    c1 = 0  # road\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i][0] == 1:\n",
    "            c0 = c0 + 1\n",
    "        else:\n",
    "            c1 = c1 + 1\n",
    "    print('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "    print('Balancing training data...')\n",
    "    min_c = min(c0, c1)\n",
    "    idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "    idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "    new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "    print(len(new_indices))\n",
    "    print(train_data.shape)\n",
    "    train_data = train_data[new_indices, :, :, :]\n",
    "    train_labels = train_labels[new_indices]\n",
    "    train_size = train_labels.shape[0]\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    for i in range(len(train_labels)):\n",
    "        if train_labels[i][0] == 1:\n",
    "            c0 = c0 + 1\n",
    "        else:\n",
    "            c1 = c1 + 1\n",
    "    print('Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1))\n",
    "\n",
    "    # This is where training samples and labels are fed to the graph.\n",
    "    # These placeholder nodes will be fed a batch of training data at each\n",
    "    # training step using the {feed_dict} argument to the Run() call below.\n",
    "    train_data_node = tf.placeholder(\n",
    "        tf.float32,\n",
    "        shape=(BATCH_SIZE, IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS))\n",
    "    train_labels_node = tf.placeholder(tf.float32,\n",
    "                                       shape=(BATCH_SIZE, NUM_LABELS))\n",
    "    train_all_data_node = tf.constant(train_data)\n",
    "\n",
    "    # The variables below hold all the trainable weights. They are passed an\n",
    "    # initial value which will be assigned when when we call:\n",
    "    # {tf.initialize_all_variables().run()}\n",
    "    conv1_weights = tf.Variable(\n",
    "        tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                            stddev=0.1,\n",
    "                            seed=SEED))\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "    conv2_weights = tf.Variable(\n",
    "        tf.truncated_normal([5, 5, 32, 64],\n",
    "                            stddev=0.1,\n",
    "                            seed=SEED))\n",
    "    conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "    fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "        tf.truncated_normal([int(IMG_PATCH_SIZE / 4 * IMG_PATCH_SIZE / 4 * 64), 512],\n",
    "                            stddev=0.1,\n",
    "                            seed=SEED))\n",
    "    fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "    fc2_weights = tf.Variable(\n",
    "        tf.truncated_normal([512, NUM_LABELS],\n",
    "                            stddev=0.1,\n",
    "                            seed=SEED))\n",
    "    fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "    # Make an image summary for 4d tensor image with index idx\n",
    "    def get_image_summary(img, idx=0):\n",
    "        V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
    "        img_w = img.get_shape().as_list()[1]\n",
    "        img_h = img.get_shape().as_list()[2]\n",
    "        min_value = tf.reduce_min(V)\n",
    "        V = V - min_value\n",
    "        max_value = tf.reduce_max(V)\n",
    "        V = V / (max_value*PIXEL_DEPTH)\n",
    "        V = tf.reshape(V, (img_w, img_h, 1))\n",
    "        V = tf.transpose(V, (2, 0, 1))\n",
    "        V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "        return V\n",
    "    \n",
    "    # Make an image summary for 3d tensor image with index idx\n",
    "    def get_image_summary_3d(img):\n",
    "        V = tf.slice(img, (0, 0, 0), (1, -1, -1))\n",
    "        img_w = img.get_shape().as_list()[1]\n",
    "        img_h = img.get_shape().as_list()[2]\n",
    "        V = tf.reshape(V, (img_w, img_h, 1))\n",
    "        V = tf.transpose(V, (2, 0, 1))\n",
    "        V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "        return V\n",
    "\n",
    "    # Get prediction for given input image \n",
    "    def get_prediction(img):\n",
    "        data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "        data_node = tf.constant(data)\n",
    "        output = tf.nn.softmax(model(data_node))\n",
    "        output_prediction = s.run(output)\n",
    "        img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "        return img_prediction\n",
    "\n",
    "    # Get a concatenation of the prediction and groundtruth for given input file\n",
    "    def get_prediction_with_groundtruth(filename, image_idx):\n",
    "        imageid = \"satImage_%.3d\" % image_idx\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        img = mpimg.imread(image_filename)\n",
    "        img_prediction = get_prediction(img)\n",
    "        cimg = concatenate_images(img, img_prediction)\n",
    "        return cimg\n",
    "\n",
    "    # Get prediction overlaid on the original image for given input file\n",
    "    def get_prediction_with_overlay(filename, image_idx):\n",
    "        imageid = \"satImage_%.3d\" % image_idx\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        img = mpimg.imread(image_filename)\n",
    "        img_prediction = get_prediction(img)\n",
    "        oimg = make_img_overlay(img, img_prediction)\n",
    "        return oimg\n",
    "\n",
    "    # We will replicate the model structure for the training subgraph, as well\n",
    "    # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "    def model(data, train=False):\n",
    "        \"\"\"The Model definition.\"\"\"\n",
    "        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "        # the same size as the input). Note that {strides} is a 4D array whose\n",
    "        # shape matches the data layout: [image index, y, x, depth].\n",
    "        conv = tf.nn.conv2d(data,\n",
    "                            conv1_weights,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding='SAME')\n",
    "        # Bias and rectified linear non-linearity.\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "        # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "        pool = tf.nn.max_pool(relu,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding='SAME')\n",
    "        conv2 = tf.nn.conv2d(pool,\n",
    "                             conv2_weights,\n",
    "                             strides=[1, 1, 1, 1],\n",
    "                             padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "        pool2 = tf.nn.max_pool(relu2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "        # Uncomment these lines to check the size of each layer\n",
    "        # print 'data ' + str(data.get_shape())\n",
    "        # print 'conv ' + str(conv.get_shape())\n",
    "        # print 'relu ' + str(relu.get_shape())\n",
    "        # print 'pool ' + str(pool.get_shape())\n",
    "        # print 'pool2 ' + str(pool2.get_shape())\n",
    "\n",
    "        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "        # fully connected layers.\n",
    "        pool_shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(\n",
    "            pool2,\n",
    "            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "        # Fully connected layer. Note that the '+' operation automatically\n",
    "        # broadcasts the biases.\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        # Add a 50% dropout during training only. Dropout also scales\n",
    "        # activations such that no rescaling is needed at evaluation time.\n",
    "        #if train:\n",
    "        #    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "        out = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "        if train:\n",
    "            summary_id = '_0'\n",
    "            s_data = get_image_summary(data)\n",
    "            tf.summary.image('summary_data' + summary_id, s_data, max_outputs=3)\n",
    "            s_conv = get_image_summary(conv)\n",
    "            tf.summary.image('summary_conv' + summary_id, s_conv, max_outputs=3)\n",
    "            s_pool = get_image_summary(pool)\n",
    "            tf.summary.image('summary_pool' + summary_id, s_pool, max_outputs=3)\n",
    "            s_conv2 = get_image_summary(conv2)\n",
    "            tf.summary.image('summary_conv2' + summary_id, s_conv2, max_outputs=3)\n",
    "            s_pool2 = get_image_summary(pool2)\n",
    "            tf.summary.image('summary_pool2' + summary_id, s_pool2, max_outputs=3)\n",
    "        return out\n",
    "\n",
    "    # Training computation: logits + cross-entropy loss.\n",
    "    logits = model(train_data_node, True)  # BATCH_SIZE*NUM_LABELS\n",
    "    # print 'logits = ' + str(logits.get_shape()) + ' train_labels_node = ' + str(train_labels_node.get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_labels_node,\n",
    "                                                   logits=logits))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    all_params_node = [conv1_weights, conv1_biases, conv2_weights, conv2_biases, fc1_weights, fc1_biases, fc2_weights, fc2_biases]\n",
    "    all_params_names = ['conv1_weights', 'conv1_biases', 'conv2_weights', 'conv2_biases', 'fc1_weights', 'fc1_biases', 'fc2_weights', 'fc2_biases']\n",
    "    all_grads_node = tf.gradients(loss, all_params_node)\n",
    "    all_grad_norms_node = []\n",
    "    for i in range(0, len(all_grads_node)):\n",
    "        norm_grad_i = tf.global_norm([all_grads_node[i]])\n",
    "        all_grad_norms_node.append(norm_grad_i)\n",
    "        tf.summary.scalar(all_params_names[i], norm_grad_i)\n",
    "    \n",
    "    # L2 regularization for the fully connected parameters.\n",
    "    regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                    tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "    # Add the regularization term to the loss.\n",
    "    loss += 5e-4 * regularizers\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and\n",
    "    # controls the learning rate decay.\n",
    "    batch = tf.Variable(0)\n",
    "    # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "        train_size,          # Decay step.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    # tf.scalar_summary('learning_rate', learning_rate)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    \n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                           0.0).minimize(loss,\n",
    "                                                         global_step=batch)\n",
    "\n",
    "    # Predictions for the minibatch, validation set and test set.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    # We'll compute them only once in a while by calling their {eval()} method.\n",
    "    train_all_prediction = tf.nn.softmax(model(train_all_data_node))\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a local session to run this computation.\n",
    "    with tf.Session() as s:\n",
    "        if RESTORE_MODEL:\n",
    "            # Restore variables from disk.\n",
    "            saver.restore(s, FLAGS.train_dir + \"/model.ckpt\")\n",
    "            print(\"Model restored.\")\n",
    "        else:\n",
    "            # Run all the initializers to prepare the trainable parameters.\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Build the summary operation based on the TF collection of Summaries.\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            summary_writer = tf.summary.FileWriter(FLAGS.train_dir,\n",
    "                                                   graph=s.graph)\n",
    "            print('Initialized!')\n",
    "            # Loop through training steps.\n",
    "            print('Total number of iterations = ' + str(int(num_epochs * train_size / BATCH_SIZE)))\n",
    "            training_indices = range(train_size)\n",
    "            for iepoch in range(num_epochs):\n",
    "                # Permute training indices\n",
    "                perm_indices = numpy.random.permutation(training_indices)\n",
    "                steps_per_epoch = int(train_size / BATCH_SIZE)\n",
    "                for step in range(steps_per_epoch):\n",
    "                    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "                    batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]\n",
    "                    # Compute the offset of the current minibatch in the data.\n",
    "                    # Note that we could use better randomization across epochs.\n",
    "                    batch_data = train_data[batch_indices, :, :, :]\n",
    "                    batch_labels = train_labels[batch_indices]\n",
    "                    # This dictionary maps the batch data (as a numpy array) to the\n",
    "                    # node in the graph is should be fed to.\n",
    "                    feed_dict = {train_data_node: batch_data,\n",
    "                                 train_labels_node: batch_labels}\n",
    "                    if step == 0:\n",
    "                        summary_str, _, l, lr, predictions = s.run(\n",
    "                            [summary_op, optimizer, loss, learning_rate, train_prediction],\n",
    "                            feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_str, iepoch * steps_per_epoch)\n",
    "                        summary_writer.flush()\n",
    "                        print('Epoch %d' % iepoch)\n",
    "                        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "                        print('Minibatch error: %.1f%%' % error_rate(predictions,\n",
    "                                                                     batch_labels))\n",
    "                        sys.stdout.flush()\n",
    "                    else:\n",
    "                        # Run the graph and fetch some of the nodes.\n",
    "                        _, l, lr, predictions = s.run(\n",
    "                            [optimizer, loss, learning_rate, train_prediction],\n",
    "                            feed_dict=feed_dict)\n",
    "                # Save the variables to disk.\n",
    "                save_path = saver.save(s, FLAGS.train_dir + \"/model.ckpt\")\n",
    "                print(\"Model saved in file: %s\" % save_path)\n",
    "        print(\"Running prediction on training set\")\n",
    "        prediction_training_dir = \"predictions_training/\"\n",
    "        if not os.path.isdir(prediction_training_dir):\n",
    "            os.mkdir(prediction_training_dir)\n",
    "        for i in range(1, TRAINING_SIZE + 1):\n",
    "            pimg = get_prediction_with_groundtruth(train_data_filename, i)\n",
    "            Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "            oimg = get_prediction_with_overlay(train_data_filename, i)\n",
    "            oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'verbosity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/cs55qw2j2hq1gh73rh2_bhj00000gn/T/ipykernel_70217/66951554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    290\u001b[0m   \"\"\"\n\u001b[1;32m    291\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     args = _run_init(\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mflags_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_init\u001b[0;34m(argv, flags_parser)\u001b[0m\n\u001b[1;32m    358\u001b[0m   \u001b[0mcommand_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_process_name_useful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[0;31m# Set up absl logging handler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_absl_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m   args = _register_and_parse_flags_with_usage(\n\u001b[1;32m    362\u001b[0m       \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/absl/logging/__init__.py\u001b[0m in \u001b[0;36muse_absl_handler\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1217\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mabsl_handler\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsl_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verbosity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_logging_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logger_levels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_logger_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    468\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m\"\"\"Returns the Flag object for the flag --name.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_hide_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'verbosity'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
